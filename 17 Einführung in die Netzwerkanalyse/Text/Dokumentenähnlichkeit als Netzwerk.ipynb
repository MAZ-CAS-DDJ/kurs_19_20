{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netzwerk aus Dokumenten aufgrund ihrer Ähnlichkeit\n",
    "\n",
    "- Wir probieren nun was ganz anderes. Nämlich ein Netzwerk aus Dokumenten zu bauen aufgrund ihrer Ähnlichkeit zueinander. \n",
    "- Die Idee dahinter ist Dokumente visuell zu clustern und so zu erkennnen welche ähnlichen Dokumente zusammengehören. \n",
    "- Die Idee ähnliche Dinge als Netzwerk zu visualisieren ist sehr praktisch und mächtig weil sie uns erlaubt in allen Dingen Muster zu sehen. \n",
    "- Wir probieren das mal anhand der Berichterstattung auf der Fronpage von swissinfo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artikel von der Front Page scrapen\n",
    "- Zuerst scrapen wir die Artikel von der Frontpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 100 Frontpage Items\n",
    "frontpage_url = \"https://www.swissinfo.ch/webservice/swi-eng-2.0/overview\"\n",
    "r = requests.get(frontpage_url).json()\n",
    "items = []\n",
    "urls = []\n",
    "for item in r[\"page\"][\"item\"][0][\"content\"]:\n",
    "    try:\n",
    "        tmp = requests.get(\"https://www.swissinfo.ch/webservice/swi-eng-2.0/detail%s\" % item[\"url\"])\n",
    "        text = BeautifulSoup(tmp.json()[\"htmldetail\"], \"lxml\").get_text()\n",
    "        items.append(text)\n",
    "        urls.append(item[\"canonical\"])\n",
    "        print(\"Done: %s\" % item[\"url\"])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[1][0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dokumentenähnlichkeit bestimmen\n",
    "- Work tokenization\n",
    "- Stemming\n",
    "- Stopword removal\n",
    "- (Clustering with k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "\n",
    "def word_tokenizer(text):\n",
    "        #tokenizes and stems the text\n",
    "        tokens = word_tokenize(text)\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('english')]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def cluster_items(items, nb_of_clusters=5,mds=False):\n",
    "        tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n",
    "                                        stop_words=stopwords.words('english'),\n",
    "                                        lowercase=True)\n",
    "        #builds a tf-idf matrix for the sentences\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(items)\n",
    "        \n",
    "        # Show matrix\n",
    "        pairwise_similarity = tfidf_matrix * tfidf_matrix.T\n",
    "        ax = sns.heatmap(pairwise_similarity.A)\n",
    "        ax.plot()\n",
    "        \n",
    "        # Clustering\n",
    "        kmeans = KMeans(n_clusters=nb_of_clusters)\n",
    "        kmeans.fit(tfidf_matrix)\n",
    "        clusters = collections.defaultdict(list)\n",
    "        for i, label in enumerate(kmeans.labels_):\n",
    "                clusters[label].append(i)\n",
    "        if mds:\n",
    "            return [pairwise_similarity,dict(clusters)]\n",
    "        else:\n",
    "            return dict(clusters)\n",
    "\n",
    "\n",
    "#items = [\"Nature is beautiful\",\"I like green apples\", \"We should protect the trees\",\"Fruit trees provide fruits\",\"Green apples are tasty\"]\n",
    "\n",
    "#nclusters= 3\n",
    "#parwise_similarity, clusters = cluster_items(items, nclusters,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baby example: Wie funktioniert das nochmal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie sieht das Resultat aus für:\n",
    "- 2 Sätze mit nature\n",
    "- 2 Sätze mit apple\n",
    "- 1 Satz mit apple and nature\n",
    "- 1 Satz mit computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclusters = 3\n",
    "baby_items = [\"Nature is beautiful\",\n",
    "              \"We should protect nature\",\n",
    "              \"I like green apples\", \n",
    "              \"Green apples are tasty\",\n",
    "              \"Apples and nature are fun.\",\n",
    "              \"Technical sentence with computers.\"]\n",
    "result,clusters = cluster_items(baby_items,nclusters,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(nclusters):\n",
    "        print \"cluster \",cluster,\":\"\n",
    "        for i,item in enumerate(clusters[cluster]):\n",
    "                print \"\\titem \",i,\": \",baby_items[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jetzt die Ähnlichkeit der Artikel auf der Frontpage bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result,clusters = cluster_items(items,nclusters,True)\n",
    "for cluster in range(nclusters):\n",
    "        print \"cluster \",cluster,\":\"\n",
    "        for i,item in enumerate(clusters[cluster]):\n",
    "                print \"\\turl \",urls[i].split(\"/\")[-3],\": \",urls[i].split(\"/\")[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "# Als Netzwerk exportieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G  = nx.Graph(name=\"Texts\")\n",
    "\n",
    "for url in urls:\n",
    "    url_clean = url.split(\"/\")[-2]\n",
    "    G.add_node(url, size=1, label=url)\n",
    "\n",
    "entries = result.todense().tolist()\n",
    "\n",
    "for i,row in enumerate(urls):\n",
    "    row = row.split(\"/\")[-2]\n",
    "    for j,col in enumerate(urls):\n",
    "        col = col.split(\"/\")[-2]\n",
    "        if entries[i][j] > 0.2:\n",
    "            G.add_edge(row,col,weight=entries[i][j])\n",
    "\n",
    "nx.write_gexf(G,\"Texts.gexf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
